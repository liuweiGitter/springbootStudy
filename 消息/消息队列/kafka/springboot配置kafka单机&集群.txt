1.确定版本关系
springboot、spring-kafka、kafka-client三者之间有版本匹配关系要求，具体参见
https://spring.io/projects/spring-kafka

springboot   spring-kafka kafka
2.4.x        3.3.x        2.4.0
2.3.x        3.2.x        2.3.1
2.2.x        3.1.x        2.0.1/2.1.x/2.2.x

实际上三者之间的版本对应关系非常乱，官网只是给出了建议而已，实际可能无法使用

以springboot 2.2版本为例，官网建议
2.2.x uses the 2.0.x kafka-clients by default. When overriding the kafka-clients for 2.2.x see the documentation appendix.
Spring Boot 2.2 users should use 2.3.x (Boot dependency management will use the correct version).
即2.2版本的springboot应使用2.3版本的spring-kafka，并默认支持2.0版本的kafka-clients
如果此时需要支持2.2版本的kafka-clients，需要有额外配置
https://docs.spring.io/spring-kafka/docs/2.2.1.BUILD-SNAPSHOT/reference/html/deps-for-21x.html

下文版本关系为：springboot2.2.2、spring-kafka2.3.4、kafka-clients2.2.0

2.pom配置kafka依赖
根据实际的kafka版本，配置对应的依赖版本(注意，版本不匹配可能造成无法接发消息，甚至都无法启动springboot)
	<parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.2.2.RELEASE</version>
        <relativePath/>
    </parent>
	
	<!-- https://mvnrepository.com/artifact/org.springframework.kafka/spring-kafka -->
	<dependency>
		<groupId>org.springframework.kafka</groupId>
		<artifactId>spring-kafka</artifactId>
		<version>2.3.4.RELEASE</version>
	</dependency>
	
	<dependency>
		<groupId>org.apache.kafka</groupId>
		<artifactId>kafka-clients</artifactId>
		<version>2.2.0</version>
	</dependency>
	
3.application配置文件
	#kafka服务
	#单机spring.kafka.bootstrap-servers=x.x.x.x:19092
	#集群spring.kafka.bootstrap-servers=x.x.x.x:19092,x.x.x.x:19092,x.x.x.x:19092
	#生产者
	spring.kafka.producer.retries=3
	spring.kafka.producer.batch-size=10240
	spring.kafka.producer.buffer-memory=1024000
	spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
	spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
	#消费者
	spring.kafka.consumer.enable-auto-commit=true
	spring.kafka.consumer.auto-commit-interval=1000
	spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
	spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
	#监听
	#监听的主题不存在时，不报错，继续启动，默认停止启动
	spring.kafka.listener.missing-topics-fatal=false

4.消费者demo
	package com.jshx.zq.p2p.mq.consumer;

	import lombok.extern.slf4j.Slf4j;
	import org.apache.kafka.clients.consumer.ConsumerRecord;
	import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;
	import org.springframework.context.annotation.PropertySource;
	import org.springframework.kafka.annotation.KafkaListener;
	import org.springframework.stereotype.Component;

	/**
	 * @author liuwei
	 * @date 2020-03-09 22:45
	 * @desc 消费者测试
	 */
	@Component
	@ConditionalOnExpression("${kafka_start_flag:true}")
	@Slf4j
	@PropertySource(value = "classpath:/data/properties/kafka_topic.properties")
	public class TestConsumer {

		/**
		 * topics 主题，必填
		 * groupId 消费组id，必填，一个主题的消息只能被同一个组的唯一一个消费者消费
		 *      同一主题的消息将被不同组的消费者同时消费
		 *      即，消费时以组为单位，每个组都会且仅会消费一次消息
		 * @param record
		 */
		@KafkaListener(topics = "${test}",groupId = "consumer1")
		public void consumer1 (ConsumerRecord<?, ?> record) {
			log.info("consumer1>>>topic="+record.topic()+",offset="+record.offset()+",value="+record.value());
		}

		@KafkaListener(topics = "${test}",groupId = "consumer2")
		public void consumer2 (ConsumerRecord<?, ?> record) {
			log.info("consumer2>>>topic="+record.topic()+",offset="+record.offset()+",value="+record.value());
		}

	}



5.启动springboot
成功启动后有类似以下信息打印
	00:26:54.042 [RMI TCP Connection(3)-127.0.0.1] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [x.x.x.x:19092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	
	如果有生产者生产消息，正常情况下，消费者测试时控制台可能打印以下信息
	consumer1>>>topic=test1,offset=5,value=nice 
	consumer2>>>topic=test1,offset=5,value=nice

6.kafka错误问题
》6.1连接错误
00:38:13.642 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] 
WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-1, groupId=consumer1] 
Connection to node -1 (/x.x.x.x:19092) could not be established. Broker may not be available.

如果x.x.x.x为外网地址，通常这是由于kafka服务没有绑定外网地址导致的
必须在kafka配置文件中配置advertised.listeners才可以外网访问

》6.2生产者关闭错误
java.lang.NoSuchMethodError: org.apache.kafka.clients.producer.Producer.close(Ljava/time/Duration;)V
这是因为kafka-clients不同的版本中方法不同造成的，所以要严格匹配版本关系
spring-kafka2.3.4版本会调用kafka-client中的这个方法，在低版本中没有这个方法，kafka-client 2.2.0版本有此方法

7.KafkaTemplate注册
	package com.jshx.zq.p2p.config;

	import lombok.extern.slf4j.Slf4j;
	import org.apache.kafka.clients.producer.ProducerConfig;
	import org.apache.kafka.common.serialization.StringSerializer;
	import org.springframework.beans.factory.annotation.Value;
	import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;
	import org.springframework.context.annotation.Bean;
	import org.springframework.context.annotation.Configuration;
	import org.springframework.kafka.core.DefaultKafkaProducerFactory;
	import org.springframework.kafka.core.KafkaTemplate;
	import java.util.HashMap;

	/**
	 * @author liuwei
	 * @date 2020-03-10 10:17
	 * @desc kafka生产者模板配置类
	 */
	@Configuration
	@ConditionalOnExpression("${kafka_start_flag:true}")
	@Slf4j
	public class KafkaProducerConfig {

		@Value("${spring.kafka.bootstrap-servers}")
		private String bootstrap_servers_config;
		@Value("${spring.kafka.producer.retries}")
		private String retries_config;
		@Value("${spring.kafka.producer.batch-size}")
		private String batch_size_config;
		@Value("${spring.kafka.producer.buffer-memory}")
		private String buffer_memory_config;

		@Bean
		public KafkaTemplate kafkaTemplate(){
			log.info("KafkaTemplate注册");
			HashMap<String, Object> configs = new HashMap<>();
			configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,bootstrap_servers_config);
			configs.put(ProducerConfig.RETRIES_CONFIG,retries_config);
			configs.put(ProducerConfig.BATCH_SIZE_CONFIG,batch_size_config);
			configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG,buffer_memory_config);
			configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
			configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class);
			DefaultKafkaProducerFactory producerFactory = new DefaultKafkaProducerFactory(configs);
			return new KafkaTemplate(producerFactory);
		}

	}


8.生产者demo
	package com.jshx.zq.p2p.mq.producer;

	import com.jshx.zq.p2p.data.MetaDataCache;
	import lombok.extern.slf4j.Slf4j;
	import org.springframework.beans.factory.annotation.Autowired;
	import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;
	import org.springframework.kafka.core.KafkaTemplate;
	import org.springframework.stereotype.Component;

	import java.time.LocalDateTime;
	import java.time.format.DateTimeFormatter;

	/**
	 * @author liuwei
	 * @date 2020-03-10 02:06
	 * @desc 生产者测试
	 */
	@Component
	@ConditionalOnExpression("${kafka_start_flag:true}")
	@Slf4j
	public class TestProducer {

		@Autowired
		private KafkaTemplate kafkaTemplate;

		public void sendMsg(String msg){
			//如果生产者发送的主题尚未创建，kafka会自动创建该主题
			kafkaTemplate.send(MetaDataCache.KAFKA_TOPICS.getProperty("test"), msg);
		}

		public void test(){
			sendMsg("Time = "+ LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")));
		}

	}


























